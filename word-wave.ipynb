{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f04583-4402-41d0-a736-deb0a0fb5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the full dataset (or a predefined split)\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "\n",
    "# Take only a small portion manually (first 10,000 examples)\n",
    "small_wiki = wiki.select(range(10000))\n",
    "\n",
    "# Join non-empty text entries\n",
    "texts = [item[\"text\"] for item in small_wiki if item[\"text\"]]\n",
    "data = \" \".join(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830239a2-d1b8-42be-ab53-a6dcfc5cb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def simple_sentence_tokenizer(text):\n",
    "    # Split on period, exclamation, or question marks followed by a space and uppercase letter\n",
    "    sentence_endings = re.compile(r'(?<=[.!?])\\s+(?=[A-Z])')\n",
    "    return sentence_endings.split(text)\n",
    "\n",
    "sentences = simple_sentence_tokenizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c8a083-743f-41bb-ab9c-e56ef10dae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization (with limited vocab size)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90390b1d-7a83-4973-9a46-9023b58d3a1e",
   "metadata": {},
   "source": [
    "Basically we are converting our classification problem into a supervised learning problem by forming input-output pairs for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50fe0c68-7ddf-4550-8a53-250be67ac101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input sequences: 200000\n"
     ]
    }
   ],
   "source": [
    "# Creating n-gram sequences with cap\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_sequences = []\n",
    "max_sequences = 200000\n",
    "count = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized = tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "    for i in range(1, len(tokenized)):\n",
    "        input_sequences.append(tokenized[:i+1])\n",
    "        count += 1\n",
    "        if count >= max_sequences:\n",
    "            break\n",
    "    if count >= max_sequences:\n",
    "        break\n",
    "\n",
    "print(f\"Total input sequences: {len(input_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebd3a3f-c516-4bc6-becc-c8a65d6bf63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape: (200000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Capping max_len and pad sequences\n",
    "max_len = min(max(len(x) for x in input_sequences), 100)\n",
    "padded_input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
    "print(f\"Padded shape: {padded_input_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86672148-1403-40af-8f23-1e8aab9b6370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     0,  5401,     8],\n",
       "       [    0,     0,     0, ...,  5401,     8,     7],\n",
       "       [    0,     0,     0, ...,     8,     7,   189],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,    33, 12307,  2181],\n",
       "       [    0,     0,     0, ..., 12307,  2181,    70],\n",
       "       [    0,     0,     0, ...,  2181,    70,  1665]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437d65c3-3ad8-4801-a71b-296be7c9ad5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 99)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = padded_input_sequences[:,:-1]\n",
    "y = padded_input_sequences[:,-1]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f8e9e85-057d-4ad8-8fc1-337d2223eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Bidirectional, LSTM, Dense, Attention, Concatenate, Layer\n",
    ")\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, SparseTopKCategoricalAccuracy\n",
    "from tensorflow.keras.layers import LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c963ae0-bf86-4cb6-9cfa-fd4c81bd89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 20000  # Must match tokenizer\n",
    "embedding_dim = 128\n",
    "lstm_units = 128\n",
    "max_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50b48814-37fe-4ef1-8417-4b02903e2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "inputs = Input(shape=(max_len,), name=\"input\")\n",
    "\n",
    "# Embedding\n",
    "x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
    "\n",
    "# BiLSTM\n",
    "x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "\n",
    "# Attention Mechanism\n",
    "# We'll use built-in Additive Attention\n",
    "attention = Attention(name=\"attention_layer\")([x, x])  # Query and Value are both `x`\n",
    "x = Concatenate()([x, attention])\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "# Optional Dense layers\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# Output\n",
    "outputs = Dense(vocab_size, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e78b808-fdb6-4a0c-ae21-13ee9648c1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> â”‚ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> â”‚ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ attention_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ attention_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> â”‚ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚ global_average_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)             â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,300,000</span> â”‚ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input (\u001b[38;5;33mInputLayer\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚       \u001b[38;5;34m2,560,000\u001b[0m â”‚ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m256\u001b[0m)           â”‚         \u001b[38;5;34m263,168\u001b[0m â”‚ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ attention_layer (\u001b[38;5;33mAttention\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m256\u001b[0m)           â”‚               \u001b[38;5;34m0\u001b[0m â”‚ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m512\u001b[0m)           â”‚               \u001b[38;5;34m0\u001b[0m â”‚ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ attention_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m512\u001b[0m)           â”‚           \u001b[38;5;34m1,024\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚          \u001b[38;5;34m65,664\u001b[0m â”‚ layer_normalization[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚           \u001b[38;5;34m8,256\u001b[0m â”‚ global_average_pooling1d[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)             â”‚       \u001b[38;5;34m1,300,000\u001b[0m â”‚ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,112</span> (16.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,198,112\u001b[0m (16.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,112</span> (16.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,198,112\u001b[0m (16.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=[\n",
    "        SparseCategoricalAccuracy(name='accuracy'),\n",
    "        SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy')\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b713c8c-8c14-41f0-be2a-c92e2b1ca775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1099s\u001b[0m 699ms/step - accuracy: 0.0744 - loss: 7.0667 - top_5_accuracy: 0.2303\n",
      "Epoch 2/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m934s\u001b[0m 598ms/step - accuracy: 0.0870 - loss: 6.5469 - top_5_accuracy: 0.2403\n",
      "Epoch 3/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m936s\u001b[0m 599ms/step - accuracy: 0.1239 - loss: 6.2388 - top_5_accuracy: 0.2710\n",
      "Epoch 4/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m889s\u001b[0m 569ms/step - accuracy: 0.1443 - loss: 5.9839 - top_5_accuracy: 0.2906\n",
      "Epoch 5/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m882s\u001b[0m 564ms/step - accuracy: 0.1607 - loss: 5.7349 - top_5_accuracy: 0.3074\n",
      "Epoch 6/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m879s\u001b[0m 562ms/step - accuracy: 0.1733 - loss: 5.5058 - top_5_accuracy: 0.3230\n",
      "Epoch 7/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m894s\u001b[0m 572ms/step - accuracy: 0.1811 - loss: 5.3186 - top_5_accuracy: 0.3328\n",
      "Epoch 8/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m913s\u001b[0m 584ms/step - accuracy: 0.1899 - loss: 5.1377 - top_5_accuracy: 0.3433\n",
      "Epoch 9/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1046s\u001b[0m 669ms/step - accuracy: 0.1963 - loss: 4.9825 - top_5_accuracy: 0.3525\n",
      "Epoch 10/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 357ms/step - accuracy: 0.2034 - loss: 4.8245 - top_5_accuracy: 0.3626\n",
      "Epoch 11/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 367ms/step - accuracy: 0.2088 - loss: 4.6894 - top_5_accuracy: 0.3711\n",
      "Epoch 12/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m582s\u001b[0m 373ms/step - accuracy: 0.2157 - loss: 4.5526 - top_5_accuracy: 0.3827\n",
      "Epoch 13/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 369ms/step - accuracy: 0.2238 - loss: 4.4227 - top_5_accuracy: 0.3946\n",
      "Epoch 14/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 375ms/step - accuracy: 0.2303 - loss: 4.3075 - top_5_accuracy: 0.4067\n",
      "Epoch 15/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 378ms/step - accuracy: 0.2383 - loss: 4.1877 - top_5_accuracy: 0.4215\n",
      "Epoch 16/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m597s\u001b[0m 382ms/step - accuracy: 0.2464 - loss: 4.0781 - top_5_accuracy: 0.4340\n",
      "Epoch 17/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 386ms/step - accuracy: 0.2551 - loss: 3.9835 - top_5_accuracy: 0.4474\n",
      "Epoch 18/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 392ms/step - accuracy: 0.2647 - loss: 3.8810 - top_5_accuracy: 0.4629\n",
      "Epoch 19/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 396ms/step - accuracy: 0.2748 - loss: 3.7871 - top_5_accuracy: 0.4760\n",
      "Epoch 20/20\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 392ms/step - accuracy: 0.2843 - loss: 3.6880 - top_5_accuracy: 0.4894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20c4545f7a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "batch_size = 128\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.shuffle(buffer_size=10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train\n",
    "model.fit(dataset, epochs=20, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2210cd6c-76ef-4127-8578-f708bfe5b2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 142ms/step - accuracy: 0.2230 - loss: 6.0284 - top_5_accuracy: 0.3717\n",
      "loss: 5.3835\n",
      "compile_metrics: 0.2596\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(dataset, verbose=1)\n",
    "\n",
    "# Print metrics\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4379b75-c06d-4b4f-a99a-c1d5ba66c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate_text(seed_text, next_words=10, max_len=50):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_len, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        predicted_id = tf.argmax(predicted_probs).numpy()\n",
    "        predicted_word = tokenizer.index_word.get(predicted_id, '')\n",
    "        seed_text += ' ' + predicted_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2fa44b9-36c5-4113-8628-f1ff2d8d67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def beam_search_decoder(seed_text, beam_width=3, next_words=10, max_len=50):\n",
    "    sequences = [(seed_text, 0.0)]  # (sequence, score)\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            tokenized = tokenizer.texts_to_sequences([seq])[0]\n",
    "            tokenized = pad_sequences([tokenized], maxlen=max_len, padding='pre')\n",
    "            preds = model.predict(tokenized, verbose=0)[0]\n",
    "            \n",
    "            top_indices = np.argsort(preds)[-beam_width:]  # top beam_width words\n",
    "            for idx in top_indices:\n",
    "                word = tokenizer.index_word.get(idx, '')\n",
    "                if not word: continue\n",
    "                candidate = (seq + ' ' + word, score - np.log(preds[idx] + 1e-10))  # use log prob\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        # Select best `beam_width` sequences\n",
    "        sequences = heapq.nsmallest(beam_width, all_candidates, key=lambda tup: tup[1])\n",
    "\n",
    "    return sequences[0][0]  # Return best scoring sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdec6b52-b5bb-4ff0-b11b-0fe4ea882f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def evaluate_bleu(reference_sentence, generated_sentence):\n",
    "    reference = [reference_sentence.split()]\n",
    "    candidate = generated_sentence.split()\n",
    "    return sentence_bleu(reference, candidate, weights=(0.5, 0.5))  # bigram BLEU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcba4895-3d37-4b5a-ae44-d6dbd779357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def compute_perplexity(model, dataset):\n",
    "    loss = model.evaluate(dataset, verbose=0)[0]\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad7ee6-8068-47f7-bdc6-945caaa87523",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"\\nEnter a seed text (or type 'exit' to quit): \").strip()\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    try:\n",
    "        next_words = int(input(\"How many words do you want to generate? (e.g., 5, 10): \"))\n",
    "    except ValueError:\n",
    "        print(\"âš ï¸ Please enter a valid number.\")\n",
    "        continue\n",
    "\n",
    "    generated_output = generate_text(user_input, next_words=next_words, max_len=X.shape[1])\n",
    "    print(f\"\\nğŸ“ Generated Text:\\n{generated_output}\")\n",
    "\n",
    "    # Ask for reference sentence for BLEU\n",
    "    reference = input(\"Enter reference sentence for BLEU score (or press Enter to skip): \").strip()\n",
    "    if reference:\n",
    "        bleu = evaluate_bleu(reference, generated_output)\n",
    "        print(f\"ğŸ”µ BLEU Score: {bleu:.4f}\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ BLEU Score skipped.\")\n",
    "\n",
    "    # Perplexity\n",
    "    perplexity = compute_perplexity(model, dataset)\n",
    "    print(f\"ğŸ“‰ Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd904575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with beam search decoding\n",
    "while True:\n",
    "    user_input = input(\"\\nEnter a seed text (or type 'exit' to quit): \").strip()\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    try:\n",
    "        next_words = int(input(\"How many words do you want to generate? (e.g., 5, 10): \"))\n",
    "    except ValueError:\n",
    "        print(\"âš ï¸ Please enter a valid number.\")\n",
    "        continue\n",
    "\n",
    "    generated_output = beam_search_decoder(user_input, beam_width=5, next_words=next_words, max_len=X.shape[1])\n",
    "    print(f\"\\nğŸ“ Generated Text:\\n{generated_output}\")\n",
    "\n",
    "    # Ask for reference sentence for BLEU\n",
    "    reference = input(\"Enter reference sentence for BLEU score (or press Enter to skip): \").strip()\n",
    "    if reference:\n",
    "        bleu = evaluate_bleu(reference, generated_output)\n",
    "        print(f\"ğŸ”µ BLEU Score: {bleu:.4f}\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ BLEU Score skipped.\")\n",
    "\n",
    "    # Perplexity\n",
    "    perplexity = compute_perplexity(model, dataset)\n",
    "    print(f\"ğŸ“‰ Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db29951-58f1-4361-9357-853fda5d2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"word-wave.keras\")\n",
    "\n",
    "# Save tokenizer\n",
    "import pickle\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
